---
title: "COVID 19 Analysis Project"
author: Cesar Hanna
output:
  html_document:
    df_print: paged
    toc: true 
    toc_depth: 4
    toc_float: true
date: "2025-01-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "https://cran.r-project.org"))
```

```{r packages, include=FALSE}
install.packages("leaflet")
install.packages("maps")
install.packages("sf")
install.packages("rnaturalearth")
install.packages("rnaturalearthdata")
```

```{r libraries, include=FALSE}
library(tidyverse)
library(lubridate)
library(scales)
library(leaflet)
library(maps)
library(sf)
library(rlang)
library(rnaturalearth)
library(rnaturalearthdata)
```

<br>

## **Introduction and Objectives**

COVID 19 was a virus that infected the globe for almost 3 years reaping a lot of lives, causing long-term illness and impacting the world's economy.

In this project, I want to study some parts of this pandemic and to understand its impact through statistics and comparisons.

The analysis part of my report, will be followed by a model that will predict the number of deaths in 1000 people sample population, in addition to an evaluation of this model's accuracy.

<br>

## **Data Exploration**

The data in this project is imported from John Hopkins GitHub site exploring the time series for global and US data. Reference link: https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series

Let's first import the data and have a look on what it entails.

```{r data import}

# Identifying the main file path:
main_path <- "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/refs/heads/master/csse_covid_19_data/csse_covid_19_time_series/"

# Getting the file names and appending them into a vector:
file_names <- c("time_series_covid19_confirmed_US.csv",
                "time_series_covid19_confirmed_global.csv",
                "time_series_covid19_deaths_US.csv",
                "time_series_covid19_deaths_global.csv")

# Concatenating the main path and each file name:
cases_us_path <- str_c(main_path, file_names[1])
cases_global_path <- str_c(main_path, file_names[2])
deaths_us_path <- str_c(main_path, file_names[3])
deaths_global_path <- str_c(main_path, file_names[4])
lookup_global_path <- "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/refs/heads/master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv"

# Reading each full path into a dataframe:
cases_us_df <- read_csv(cases_us_path, show_col_types = FALSE)
cases_global_df <- read_csv(cases_global_path, show_col_types = FALSE)
deaths_us_df <- read_csv(deaths_us_path, show_col_types = FALSE)
deaths_global_df <- read_csv(deaths_global_path, show_col_types = FALSE)
lookup_global_dataframe <- read_csv(lookup_global_path, show_col_types = FALSE)

# Displaying the dataframes:
cases_us_df
cases_global_df
deaths_us_df
deaths_global_df
lookup_global_dataframe
```

Let's see if the cases and deaths dataframes for the US have any duplicate keys.
  
```{r function to check for duplicate keys}

check_for_duplicates <- function(dataframe) {
  # Create a temporary dataframe that us used to check for duplicates in the UID:
  dup_check_df <- dataframe %>%
    count(UID)

  # Check for duplicates
  if (sum(dup_check_df$n) > length(dup_check_df$UID)) {
    print("There are duplicates")
  }
  else if (sum(dup_check_df$n) == length(dup_check_df$UID)) {
    print("No duplicates")
  } 
}
```
  
  - Cases dataframe - US:
  
```{r cases df for us}

check_for_duplicates(cases_us_df)
```
We see that the sum of the count of all the UID records is equal to the length of the UID column itself, which means that there are no duplicate UID's in the **cases_us_df** dataframe, so each record/row is unique.

  - Deaths dataframe - US:
  
```{r deaths df for us}

check_for_duplicates(deaths_us_df)
```
We see that the sum of the count of all the UID records is equal to the length of the UID column itself, which means that there are no duplicate UID's in the **deaths_us_df** dataframe, so each record/row is unique.

Next, I want to check how many countries in the global dataframe for both cases and deaths, have a state/province. This will help later improving the accuracy of certain data analysis and the model that I am aiming to create.

```{r function to check country state}

check_state <- function(dataframe) {
  state_check_df <- dataframe %>%
    #group_by(dataframe$"Country/Region", dataframe$"Province/State") %>%
    filter(is.na(dataframe$"Province/State")) %>%
    select("Province/State", "Country/Region") %>%
    distinct()
  
  return(state_check_df)
}
```


```{r cases and deaths df for global}

check_state(cases_global_df)
check_state(deaths_global_df)
```

Since both cases and deaths dataframes have the same number of records, I am going to visualize only one of them; let's take the cases:

```{r visualizing the countries with state vs no-state}

# Creating the data:
num_state <- c(91, 198)
state_status <- c("With States", "Without States")

# Creating a dataframe from the given data:
df <- data.frame(state_status, num_state)

# Creating a bar plot:
barplot(df$num_state, 
        names.arg = df$state_status, 
        col = c("steelblue", "grey"), 
        main = "State Status Comparison", 
        xlab = "State Status", 
        ylab = "Number of Countries",
        ylim = c(0, max(num_state) + 20)) # This adds some space above the highest bar

```

Since the number of countries that do not have a state is significantly more than the ones that have a state, I am going to remove the Province/State column from my analysis to avoid having imbalanced data.

Further exploration:

Now, I want to check the consistency of the data across the original dataframes and see what action(s) I should take.

First, I want to see the naming consistency for the abbreviation of the US country in all datasets:

```{r us abbreviated naming consistency check}

# Creating a function that checks if the previous Province_State records are found in the global dataframes for deaths and cases:
check_naming_consistency <- function(dataframe) {
  output_df <- dataframe %>%
    filter(`Country/Region` == "American Samoa" |
           `Country/Region` == "Guam" |
           `Country/Region` == "Northern Mariana Islands" |
           `Country/Region` == "Puerto Rico" |
           `Country/Region` == "Virgin Islands")
  
  if (nrow(output_df) == 0) {
    print("No records found")
    return(output_df)
  }
  else if (nrow(output_df) > 0) {
    print("Records found")
    return(output_df)
  }
}
```

```{r}

check_naming_consistency(cases_global_df)
check_naming_consistency(deaths_global_df)
```

So, this means that all of the states in the US are consolidated under "US" in the global dataframes; in this case I will not do any changes to the US country name since it matches in both the global and US datasets.

<br>

## **Data Wrangling**

There are quite few things to change in the original datasets in order to make them ready for analysis and modelling.

The following section will include several tidying and wrangling steps.

### **Step 1:** Pivoting and chaging column type
  
  - The dates in all of the datasets are recorded as separate columns, making tibbles extremely hard to read and to work with. For that, I will create the dates in rows associated with each record, and change the column type to "date".

```{r pivoting tables}

# Creating a function that pivots the dataframes:
pivoting_dataframe <- function(dataframe, value) {
  if (str_detect(deparse(substitute(dataframe)), "_us_") & value == "Deaths") {
    pivoted_deaths_us_df <- dataframe %>%
      pivot_longer(-c(1:12), names_to = "Date", values_to = value) %>%  # Since the deaths dataset contains one more column, which is Population
      mutate(Date = as.Date(format(as.Date(Date, format = "%m/%d/%y"), "%Y-%m-%d")))
    return(pivoted_deaths_us_df)
  }
  else if (str_detect(deparse(substitute(dataframe)), "_us_") & value == "Cases") {
    pivoted_cases_us_df <- dataframe %>%
      pivot_longer(-c(1:11), names_to = "Date", values_to = value) %>%
      mutate(Date = as.Date(format(as.Date(Date, format = "%m/%d/%y"), "%Y-%m-%d")))
    return(pivoted_cases_us_df)
  }
  else if (str_detect(deparse(substitute(dataframe)), "_global_")) {
    pivoted_global_df <- dataframe %>%
      pivot_longer(-c(1:4), names_to = "Date", values_to = value) %>%
      mutate(Date = as.Date(format(as.Date(Date, format = "%m/%d/%y"), "%Y-%m-%d")))
    return(pivoted_global_df)
  }
}
```


```{r showing the dataframes in step 1}

# Creating and showing the dataframes:
pivoted_cases_us_df <- pivoting_dataframe(cases_us_df, "Cases")
pivoted_cases_us_df
pivoted_deaths_us_df <- pivoting_dataframe(deaths_us_df, "Deaths")
pivoted_deaths_us_df
pivoted_cases_global_df <- pivoting_dataframe(cases_global_df, "Cases")
pivoted_cases_global_df
pivoted_deaths_global_df <- pivoting_dataframe(deaths_global_df, "Deaths")
pivoted_deaths_global_df
```


### **Step 2:** Removing and editing column names
  
  - In this step, I am going to:
  
    - Remove the "iso2", "iso3", as these are represented under "Country_Region".
      
    - Remove "code3", "FIPS" and "Combined_Key" as "UID" is representing the unique identifier.
        
    - Remove the "Province/State" column in the global dataframes as the majority of the recorded countries do not have a state, which creates imbalanced data.
      
    - Change the name of "Admin2" to "County".
        
    - Rename the column "Long_" in the US dataframes to "Long"
      
    - Rename the "Province_State" column in the US dataframes to "State".
        
    - Rename the "Country_Region" in the US dataframes to "Country".
        
    - Rename the "Country/Region" in the global dataframes to "Country".
    
```{r removing and renaming columns}

col_modification <- function(dataframe) {
  if (str_detect(deparse(substitute(dataframe)), "_us_")) {
    updated_us_df <- dataframe %>%
      rename("State" = "Province_State",
             "Country" = "Country_Region",
             "Long" = "Long_",
             "County" = "Admin2") %>%  # Renaming the required columns
      select(-"iso2", -"iso3", -"code3", -"FIPS", -"Combined_Key")  # Removing the "iso2", "iso3", "FIPS" and "code3" columns
    return(updated_us_df)
  }
  else if (str_detect(deparse(substitute(dataframe)), "_global_")) {
    updated_global_df <- dataframe %>%
      rename("Country" = "Country/Region") %>%
      select(-"Province/State")
    return(updated_global_df)
  }
}
```

```{r showing the dataframes in step 2}

# Creating and showing the dataframes:
updated_cases_us_df <- col_modification(pivoted_cases_us_df)
updated_cases_us_df
updated_deaths_us_df <- col_modification(pivoted_deaths_us_df)
updated_deaths_us_df
updated_cases_global_df <- col_modification(pivoted_cases_global_df)
updated_cases_global_df
updated_deaths_global_df <- col_modification(pivoted_deaths_global_df)
updated_deaths_global_df
```

### **Step 3:** Check for remaining null values
  
  - Last step is just checking if we have any NA values still in the dataframes and based on that decide what to do.
    
```{r function to check for NA values}

check_for_na <- function(dataframe) {
  count <- 0
  for (column in colnames(dataframe)) {  # Looping through the columns
    for (value in column) {  # looping through the values of each column
      if (is.na(value)) {
      count <- count + 1
      }
    }
  }
  paste("Count of NA values in", deparse(substitute(dataframe)), "is:", count)
}
```

```{r displaying results in step 3}

check_for_na(updated_cases_us_df)
check_for_na(updated_deaths_us_df)
check_for_na(updated_cases_global_df)
check_for_na(updated_deaths_global_df)
```

The data show that there are no NA values in all of the dataframes, which is a good a thing, so no further action is needed.

Now that we have the data all tidied, we are ready to do some analysis in the next section.

<br>

## **Data Analysis**

I decided in this project to conduct the following analysis, since, in my point of view, it shows fundamental comparison between states and countries for case and death averages over time. In addition, the analysis entails rankings for both cases and deaths, which give us some insights on where the virus hit, why, and what were the measures taken.

### **US and Global**
  
  1. US Counties - Comparison between cases and deaths for the Top 10 counties in each state ordered by total cases - w/ visualization.
      
  2. US - Comparison between delta cases and delta deaths on daily basis from January 1, 2020 until December 31, 2022 - w/ visualization.
      
  3. US States - Top 10 states for total cases from highest to lowest - w/ visualization.
      
  4. US - 1-day and 7-day delta average for cases and deaths from January 1, 2020 until December 31, 2022 - w/ visualization.
      
  5. US States - Delta cases and delta deaths on daily basis per state - w/ visualization
      
  6. US States - Deaths and cases per 100K people for the top 10 states - w/ visualization.
    
  7. US States - Map view of the states for total cases and deaths.
      
  8. Global - Top 10 countries with the total cases and deaths per 1k people from highest to lowest - w/ visualization.
      
  9. Global - Map view of total cases and deaths.
      
  10. Global vs US states comparison - w/ visualization.
    
    
#### **Analysis 1:** US Counties - Comparison between cases and deaths for the Top 10 counties in each state ordered by total cases - w/ visualization.
    
```{r comparison between cases and deaths - us counties}

# Joining the required dataframes:
us_cases_deaths_joined <- updated_deaths_us_df %>%
  left_join(updated_cases_us_df, by = join_by(UID, Date)) %>%
  rename("County" = "County.x",
         "State" = "State.x",
         "Country" = "Country.x",
         "Lat" = "Lat.x",
         "Long" = "Long.x") %>%
  select(-County.y, -State.y, -Country.y, -Lat.y, -Long.y)
us_cases_deaths_joined
```

```{r visualization - analysis 1}

# Preparing the dataframe for visualization:
us_counties_cases_deaths_comp <- us_cases_deaths_joined %>%
  group_by(State, County) %>%  # This group_by is to make sure that we don't have a redundant sum of the total deaths and total cases, in which we separate them by county.
  reframe(Total_Deaths = max(Deaths),
          Total_Cases = max(Cases)) %>%
  distinct(State, County, Total_Deaths, Total_Cases, .keep_all = TRUE) %>%  # This removes the duplicate rows that have the same state, Total_Deaths and Total_Cases. It removes the second occurence and keeps the first.
  group_by(State) %>%  # This group_by wraps up the data for each state, so we can slice the record with the maximum total cases.
  slice_head(n = 10) %>%  # Getting the top 10 records for each state.
  arrange(State, desc(Total_Deaths))

# Showing the dataframe:
us_counties_cases_deaths_comp

# Let's see how many states do not have a county:
us_counties_cases_deaths_comp %>%
  filter(is.na(County))

# Replacing the county name with the state name for the states that don't have a county:
us_counties_cases_deaths_comp_updated <- us_counties_cases_deaths_comp %>%
  mutate(County = ifelse(is.na(County), State, County))

# Showing the replacement if it is successful:
us_counties_cases_deaths_comp_updated %>%
  filter(County %in% c("American Samoa", "Diamond Princess", 
                       "Grand Princess", "Guam", 
                       "Northern Mariana Islands", "Virgin Islands"))
```

```{r plotting - us_counties_cases_deaths_comp_updated, warning=FALSE}

# Plotting the updated dataframe:
filtered_states <- subset(us_counties_cases_deaths_comp_updated, State %in% c("Alabama", "Arizona", "Texas", "California", "Colorado", "Florida"))  # Since the dataset is too big to fit into a plot without showing messy data, I am taking a sample of 6 states to analyse the trend.

# Creating the required plot. I am using here a dot plot:
ggplot(filtered_states, aes(x = Total_Deaths, y = reorder(County, Total_Deaths), color = State)) +
  geom_point(size = 3) +
  facet_wrap(~ State, scales = "free_y") +
  scale_x_log10() +  # Scaling the x-axis to better fit the data, using log based 10. This scales the data down while keeping its integrity.
  labs(title = "Top Counties by Total Deaths in 6 States",
       x = "Total Deaths",
       y = "County") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 8),
        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```


We can observe from the sample above the following:
  
  - We can see that all the 6 states fall into the interval between 6K and 3.8M (un-scaled), with the lowest count 0 in California and the highest count 11M in Arizona being outliers compared to the other choosen states in this analysis.
  
  - Close similarity of total deaths between Colorado, Alabama, Texas and Arizona, with an outlier in Maricopa - Arizona.
  
  - California has more or less the same highest total deaths count as Florida, with the lowest recorded total deaths in Alpine county (0 death), among the 5 remaining states.


#### **Analysis 2:** US - Comparison between delta cases and delta deaths on daily basis from January 1, 2020 until December 31, 2022 - w/ visualization.

```{r comparison between delta cases and delta deaths - us}

# Creating the required dataframe. I am going to compute the total deaths and cases and derive the delta from those totals, while applying the required filter on the date:
us_cases_deaths_grouped <- us_cases_deaths_joined %>%
  group_by(Date) %>%
  reframe(Total_Deaths = sum(Deaths),
          Total_Cases = sum(Cases)) %>%
  filter(Date >= "2020-01-22" & Date <= "2022-12-31")

us_cases_deaths_delta <- us_cases_deaths_grouped %>%
  reframe(Date,
          Total_Deaths,
          Total_Cases,
          Delta_Deaths_1_day = Total_Deaths - lag(Total_Deaths), # delta of 1 day
          Delta_Cases_1_day = Total_Cases - lag(Total_Cases))

# Showing the dataframe:
us_cases_deaths_delta
```

```{r plottting - us_counties_cases_deaths_delta, warning=FALSE}

# Plotting the dataframe:
# Selecting the Date, Delta Deaths and Delta Cases for plotting:
selected_daily_delta_df <- us_cases_deaths_delta %>%
  select(-Total_Deaths, -Total_Cases)

# Reshaping the data to enable plotting both total death and total cases on the same graph:
df_reshaped <- selected_daily_delta_df %>%
  pivot_longer(cols = c(Delta_Deaths_1_day, Delta_Cases_1_day), names_to = "metric", values_to = "value") %>%
  filter(!is.na(value))

# Creating the logarithmic line chart:
ggplot(df_reshaped, aes(x = Date, y = value, color = metric)) +
  geom_line(size = 0.5) +
  scale_y_log10(labels = scales::comma) +
  scale_color_manual(values = c("Delta_Cases_1_day" = "skyblue", "Delta_Deaths_1_day" = "red"),
                     labels = c("Delta Cases", "Delta Deaths")) +
  labs(title = "Delta Cases and Deaths Over Time",
       x = "Date",
       y = "Count",
       color = "Legend") +
  theme_minimal()
```

To avoid having a messy plot, since we have extensive data, I used logarithm in which it reduces the scale by keeping the data integrity.

If we look at the plot:

  - We see that both cases and deaths frequency and amplitude are more or less similar.
  
  - A significant surge in cases and deaths in 2020, where the first variant of the virus was both highly infectious and deadly.
  
  - The cases and deaths kept on rising until beginning of 2021, and started to go down until the middle of the year.
  
  - In the beginning of 2022, the cases and deaths peaked again, and started to go down until the first quarter of the year, where they reached a low count.
  
  - After the first quarter of 2022, the cases and deaths went back up until the middle of the year, with a higher amplitude of cases, and from there they stabilized going into 2023.
  
  
#### **Analysis 3:** US States - Top 10 states for total cases from highest to lowest - w/ visualization.
    
```{r top 10 states for total cases}

# Creating the required dataframe: 
top10_us_cases_deaths <- us_cases_deaths_joined %>%
  group_by(State, Date) %>%  # This group_by is to make sure that we don't have a redundant sum of the total deaths and total cases, in which we separate them by date.
  reframe(Total_Deaths = sum(Deaths),
          Total_Cases = sum(Cases)) %>%
  distinct(State, Total_Deaths, Total_Cases, .keep_all = TRUE) %>%  # This removes the duplicate rows that have the same state, Total_Deaths and Total_Cases. It removes the second occurence and keeps the first.
  group_by(State) %>%  # This group_by wraps up the data for each state, so we can slice the record with the maximum total cases.
  slice_max(Total_Cases, n=1) %>%  # # This takes the maximum/highest total_cases for each state and ignores the rest of the rows for the same state.
  arrange(desc(Total_Cases))
top10_us_cases_deaths
```
```{r visualization of the top 10 states - analysis 3}

# Plotting only the top 10 states:
ggplot(head(top10_us_cases_deaths, 10), aes(x = reorder(State, -Total_Cases))) +
  geom_bar(aes(y = Total_Cases, fill = "Cases"), stat = "identity") +
  geom_bar(aes(y = Total_Deaths, fill = "Deaths"), stat = "identity") +
  scale_fill_manual(values = c("Cases" = "skyblue", "Deaths" = "red")) +
  labs(x = "State", 
       y = "Count", 
       title = "Total Cases and Deaths by State",
       fill = "Metric") +
  theme_minimal() +
  coord_flip()
```

The above graph shows the significant difference between cases and deaths being very low for the latter in comparison.

Although the number of cases varies between the states, as we can see, with Michigan, Georgia, Ohio, North Carolina and Pennsylvania being close, the number of total deaths is almost identical across all top 10 states.


#### **Analysis 4:** US - 1-day and 7-day delta average for cases and deaths from January 1, 2020 until December 31, 2022 - w/ visualization.

```{r comparison 7-day avg per 100k - us}

# Creating the required dataframe. For the 7-day average calculation, I am using the lag() function to capture the previous days:
us_cases_deaths_stats_df <- us_cases_deaths_delta %>%
  ungroup() %>%  # Using ungroup() to keep reframe() function next from grouping the date column.
  reframe(Date,
        Total_Deaths,
        Total_Cases,
        Delta_Deaths_1_day,
        Delta_Cases_1_day,
        Delta_Deaths_7_day = round(Delta_Deaths_1_day + (lag(Delta_Deaths_1_day, n=1) + lag(Delta_Deaths_1_day, n=2) +
          lag(Delta_Deaths_1_day, n=3) + lag(Delta_Deaths_1_day, n=4) +
          lag(Delta_Deaths_1_day, n=5) + lag(Delta_Deaths_1_day, n=6))/7, 2),
        Delta_Cases_7_day = round(Delta_Cases_1_day + (lag(Delta_Cases_1_day, n=1) + lag(Delta_Cases_1_day, n=2) +
          lag(Delta_Cases_1_day, n=3) + lag(Delta_Cases_1_day, n=4) +
          lag(Delta_Cases_1_day, n=5) + lag(Delta_Cases_1_day, n=6))/7, 2))

# Showing the dataframe:
us_cases_deaths_stats_df
```

For the visualization, I will take a subset of records from January 01, 2021 until December 12, 2022 just to avoid having a messy plot. This subset is a good representation of the data trend that we can see in the complete dataset.

```{r visualization-1 - analysis 4, warning=FALSE}

# Creating a bar plot for the daily cases from 2020 untiol 2023:
ggplot(us_cases_deaths_stats_df, aes(x = Date, y = Delta_Cases_1_day)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  scale_y_continuous(labels = comma) +
  labs(
    title = "Daily Cases Over Time",
    x = "Date",
    y = "Daily Cases"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

We see in the above bar plot that there was a peak in the number of daily cases by the end of 2020 - beginning of 2021.

It reached the lowest by mid-2021, peaked again in the last quarter of the same year.

Beginning of 2022, we see a significant peak in the daily cases reaching approximately 1.3M cases across all the states.

After that, the cases went down an we started seeing a fluctuation in the numbers going up and down, but not exceeding 240K cases per day.

```{r visualization-2 - analysis 4, warning=FALSE}

# Creating a scatter plot showing the ratio of deaths to cases:
ggplot(us_cases_deaths_stats_df, aes(x = Delta_Cases_1_day, y = Delta_Deaths_1_day)) +
  geom_point(alpha = 0.7, color = "darkred") +
  scale_x_continuous(labels = comma) +
  labs(
    title = "Daily Ratio - Cases vs Deaths",
    x = "Daily Cases",
    y = "Daily Deaths"
  ) +
  theme_minimal()
```

The above scatter plot shows clearly the clustering of deaths vs cases in the beginning with a steep incline, indicating that the ratio of deaths to cases was very high as the pendemic started.

The points afterwards, started to get far from each other with a declined trajectory and we couldn't see a cluster anymore, indicating that the ratio of deaths to cases went down, and that is obvious as vaccines were developed thus the number of daily deaths went down drastically.


#### **Analysis 5:** US States - Delta cases and delta deaths on daily basis per state - w/ visualization.

```{r delta cases and delta deaths on daily basis per state}

# Creating a dataframe with the states population which will be used for stats calculation later:
states_pop_df <- us_cases_deaths_joined %>%
  distinct(UID, Population, .keep_all = TRUE) %>%  # Removing duplicated records per UID and Population.
  group_by(State, Date) %>%
  reframe(Pop = sum(Population)) %>%
  select(-Date)

# Creating a function that builds the required dataframe by the state's name:
search_by_state <- function(state_name) {
  filtered_by_state_dataframe <- us_cases_deaths_joined %>%
    filter(State == state_name) %>%
    group_by(Date) %>%
    mutate(Total_Deaths = sum(Deaths),
           Total_Cases = sum(Cases)) %>%
    select(State, Date, Total_Deaths, Total_Cases)
  
  # Defining the population per state just in case this dataframe is needed for other analysis:
  state_pop_dataframe <- filtered_by_state_dataframe %>%
    left_join(states_pop_df, by = join_by(State)) %>%
    rename("Population" = "Pop") %>%
    relocate(Population, .after = "Date")
  
  # Creating the required dataframe entailing the delta deaths and cases per day:
  state_stats_df <- state_pop_dataframe %>%
    ungroup() %>%
    reframe(State,
            Date,
            Total_Deaths,
            Total_Cases,
            Delta_Deaths_1_day = Total_Deaths - lag(Total_Deaths),
            Delta_Cases_1_day = Total_Cases - lag(Total_Cases))
  
  return(state_stats_df)
}
```

```{r creating the delta deaths and cases per state dataframe}

# Defining the state and creating the required dataframe:
state_analysis_df <- search_by_state("California")
state_analysis_df

state_analysis_df <- search_by_state("Colorado")
state_analysis_df

state_analysis_df <- search_by_state("New York")
state_analysis_df
```

```{r visualization of delta cases and deaths per state, warning=FALSE}

# Creating a function that takes a dataframe and state name and builds a graph: 
build_graph_by_state <- function(state_name) {
  # Creating the needed dataframe to be plotted by calling the search_by_state function:
  delta_1_day_df <- search_by_state(state_name)
  
  # Reshaping the dataframe in order to plot the needed metrics:
  df_reshaped <- delta_1_day_df %>%
  select(-Total_Deaths, -Total_Cases) %>%
  pivot_longer(cols = c(Delta_Deaths_1_day, Delta_Cases_1_day), names_to = "metric", values_to = "value") %>%
  filter(!is.na(value))

  # Creating the logarithmic line chart:
  ggplot(df_reshaped, aes(x = Date, y = value, color = metric)) +
    geom_line(size = 0.5) +
    scale_y_log10(labels = scales::comma) +
    scale_color_manual(values = c("Delta_Cases_1_day" = "skyblue", "Delta_Deaths_1_day" = "red"),
                       labels = c("Delta Cases", "Delta Deaths")) +
    labs(title = str_c("Delta Cases and Deaths Over Time - ", state_name),
         x = "Date",
         y = "Count",
         color = "Legend") +
    theme_minimal()
}
```

```{r building the graphs, warning=FALSE}

build_graph_by_state("Colorado")
build_graph_by_state("California")
build_graph_by_state("New York")
```

Quick analysis on the above graphs:
    
  - We can see that signals of delta cases and deaths through time between those 3 states are very similar in terms of frequency and amplitude, but with different count interval - California and New York have a count interval between 0 and 100K matching these 2 states statistics , while Colorado is between 0 and 10K.
    
  - The peak of delta cases and deaths happened during the first quarter of 2020 in Colorado.
    
  - All 3 states similarly, had a surge for delta cases happened in the last quarter of 2021, and another higher one (the highest for cases) happened in the beginning of 2022, both followed by a surge of delta deaths.
    
  - The cases and deaths dropped significantly starting around mid-2022 and kept steady going towards 2023.


#### **Analysis 6:** US States - Deaths and cases per 100K people for the top 10 states - w/ visualization.

```{r deaths and cases per 100K people for the top 10 states}

# Creating the states dataframe containing the population:
states_cases_deaths_joined <- top10_us_cases_deaths %>%
  left_join(states_pop_df, by = join_by(State)) %>%
  rename("Population" = "Pop") %>%
  filter(State != "Grand Princess" & State != "Diamond Princess")  # Removing Grand Princess and Diamond Princess states as their population is recorded 0, which will produce wrong the wrong results for the deaths and cases per 100K.
states_cases_deaths_joined

# Creating the required stats dataframe containing the cases and deaths per 100K people for the top 10 states: 
states_cases_deaths_per_100K <- states_cases_deaths_joined %>%
  reframe(State,
          Date,
          Total_Deaths,
          Total_Cases,
          Population,
          Deaths_per_100K = round((Total_Deaths * 100000) / Population, 2),
          Cases_per_100K = round((Total_Cases * 100000) / Population, 2)) %>%
  arrange(desc(Total_Cases))  # Ordering the dataset by Total_Cases.

# Showing the dataframe:
states_cases_deaths_per_100K
```

```{r visualization for the top states per 100K people - analysis 5, message=FALSE, warning=FALSE}

# Preparing the dataframe for plotting:
states_cases_deaths_per_100K_selected <- states_cases_deaths_per_100K %>%
  select(-Population, - Total_Deaths, -Total_Cases)
states_cases_deaths_per_100K_selected

# Plotting the dataframe:
ggplot(states_cases_deaths_per_100K_selected, aes(x = Deaths_per_100K, y = Cases_per_100K)) +
  geom_point(alpha = 0.6) +
  labs(
    title = "Cases per 100k vs. Deaths per 100k by State",
    x = "Cases per 100k",
    y = "Deaths per 100k"
  ) +
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed", color = "black") +
  theme_minimal()
```

The graph above shows that the cases and deaths per 100K people are relatively close to the regression line.

The reason these points are close to the regression line is due to the difference ratio between the cases and deaths being approximately 1 to 1.1%, of course, with some outliers.


#### **Analysis 7:** US States - Map view of the states for total cases and deaths.
  
  - Total Deaths Map:
    
```{r map view of the states for total deaths, warning=FALSE}

# Defining the dataframe; I will use the top10_us_cases_deaths dataframe as it contains the required data:
top10_us_states_df <- top10_us_cases_deaths %>%
  select(-Date)
  
# Converting state names to lowercase to match with map data:
top10_us_states_df$State <- tolower(top10_us_states_df$State)
  
# Getting the states boundaries as an sf object:
states_sf <- st_as_sf(map("state", plot = FALSE, fill = TRUE), crs = 4326)
  
# Adding state names to the sf object:
states_sf <- states_sf %>%
  mutate(id = tolower(ID))  # Convert state names to lowercase
  
# Merging the sf object with the dataframe:
merged_data <- states_sf %>%
  left_join(top10_us_states_df, by = c("id" = "State"))
  
# Creating an interactive map:
leaflet(data = merged_data) %>%
  addTiles() %>% # Add default map tiles
  addPolygons(
    fillColor = ~colorBin("YlOrRd", Total_Deaths, bins = 5)(Total_Deaths),
    weight = 1,
    color = "black",
    fillOpacity = 0.7,
    highlight = highlightOptions(weight = 3, color = "#666", bringToFront = TRUE),
    label = ~paste("State: ", id, " ", 
                    "Total Deaths: ", round(Total_Deaths, 2), " ",
                    "Total Cases: ", round(Total_Cases, 2))
      
  ) %>%
  addLegend(
    pal = colorBin("YlOrRd", merged_data$Total_Deaths, bins = 5), 
    values = ~Total_Deaths,
    title = "Total Deaths",
    position = "bottomright"
  )
```

The above map shows the concentration of total deaths in the southern part of the country.
  
  - Total Cases Map:
      
```{r map view of the states for total cases, warning=FALSE}

# Defining the dataframe; I will use the top10_us_cases_deaths dataframe as it contains the required data:
top10_us_states_df <- top10_us_cases_deaths %>%
  select(-Date)
  
# Converting state names to lowercase to match with map data:
top10_us_states_df$State <- tolower(top10_us_states_df$State)
  
# Getting the states boundaries as an sf object:
states_sf <- st_as_sf(map("state", plot = FALSE, fill = TRUE), crs = 4326)
  
# Adding state names to the sf object:
states_sf <- states_sf %>%
  mutate(id = tolower(ID))  # Convert state names to lowercase
  
# Merging the sf object with the dataframe:
merged_data <- states_sf %>%
  left_join(top10_us_states_df, by = c("id" = "State"))
  
# Creating an interactive map:
leaflet(data = merged_data) %>%
  addTiles() %>% # Add default map tiles
  addPolygons(
    fillColor = ~colorBin("BuGn", Total_Cases, bins = 5)(Total_Cases),
    weight = 1,
    color = "black",
    fillOpacity = 0.7,
    highlight = highlightOptions(weight = 3, color = "#666", bringToFront = TRUE),
    label = ~paste("State: ", id, " ", 
                    "Total Deaths: ", round(Total_Deaths, 2), " ",
                    "Total Cases: ", round(Total_Cases, 2))
      
  ) %>%
  addLegend(
    pal = colorBin("BuGn", merged_data$Total_Cases, bins = 5), 
    values = ~Total_Cases,
    title = "Total Cases",
    position = "bottomright"
  )
```

I used here an interactive plotting for the US map where it groups the total deaths into 6 different intervals and the total cases into 7.

The total cases map shows the most cases are focused in the south, the east cost and the west cost; the numbers in the northern part are low. 

**PS:** This plot does not show the data mapped in Alaska and Hawaii as the sf package does not include any state outside of the mainland. However, looking at the data, we see that Alaskas's and Hawaii's total cases lie between 0 and 2000000 and total deaths between 0 and 20000.


#### **Analysis 8:** Global - Top 10 countries with the total cases and deaths per 1k people from highest to lowest - w/ visualization.
    
```{r global top 10 countries for total cases and deaths per 1k, warning=FALSE}

# Creating a dataframe entailing the population of all countries:
global_population_df <- lookup_global_dataframe %>%
  group_by(Country_Region) %>%
  slice_max(Population, n = 1) %>%
  select(Country_Region, Population) %>%
  rename("Country" = "Country_Region")

# Joining the wrangled global datasets entailing the cases and deaths:
global_cases_deaths_joined <- updated_deaths_global_df %>%
  full_join(updated_cases_global_df, by = join_by(Country, Date, Lat, Long))

# Creating the global dataframe entailing the population:
global_cases_deaths_with_pop <- global_cases_deaths_joined %>%
  left_join(global_population_df, by = join_by(Country)) %>%
  relocate(Population, .after = "Date") %>%
  filter(!is.na(Population))
global_cases_deaths_with_pop

# Creating a dataframe showing the total deaths and cases per country:
global_cases_deaths_grouped <- global_cases_deaths_with_pop %>%
  group_by(Country) %>%
  reframe(Population,
          Total_Deaths = max(Deaths),
          Total_Cases = max(Cases)) %>%
  distinct()
```

```{r function to build the top 10 dataframe - analysis 8}

# Creating a function that builds the top 10 dataframe based on the stats passed to it:
build_top10_dataframe <- function(dataframe, stats_column) {
  # Creating the top 10 dataframe entailing the 1-day and 7-day delta average per 1000 people:
  top10_global_per_1k <- dataframe %>%
    ungroup() %>%
    reframe(Country,
            Population,
            Total_Deaths,
            Total_Cases,
            Total_Deaths_per_1k = round((Total_Deaths * 1000) / Population, 2),
            Total_Cases_per_1k = round((Total_Cases * 1000) / Population, 2)) %>%
    arrange(desc(!!sym(stats_column)))
  
  return(top10_global_per_1k)
}
```

```{r creating and shpwing the dataframes - analysis 8}

# Top 10 per 7-day delta deaths:
top10_global_deaths_per_1k <- build_top10_dataframe(global_cases_deaths_grouped, "Total_Deaths_per_1k")
top10_global_deaths_per_1k

# Top 10 per 7-day delta cases:
top10_global_cases_per_1k <- build_top10_dataframe(global_cases_deaths_grouped, "Total_Cases_per_1k")
top10_global_cases_per_1k
```

```{r visualization - top 10 countries with 1-day and 7-day delta average for cases and deaths}

# Creating a function that plots the top 10 countries for the 7-day delta cases and deaths:
top10_global_graph_builder <- function(dataframe){
  # Remove rows with NA for the required columns:
  filtered_data <- na.omit(dataframe[, c("Total_Cases_per_1k", "Total_Deaths_per_1k")])
  
  # Creating the scatter plot for 7-day delta cases vs deaths:
  ggplot(filtered_data, aes(x = Total_Cases_per_1k, y = Total_Deaths_per_1k)) +
    geom_point(color = "blue") +
    geom_smooth(method = "lm", color = "red", se = TRUE) +
    #labs(title = "7-day Delta Cases vs Delta Deaths per 1k",
    labs(title = "Total Cases vs Total Deaths per 1k",
         x = "Total Cases per 1k",
         y = "Total Deaths per 1k") +
    theme_minimal()
}
```

```{r graph 1, message=FALSE}

top10_global_graph_builder(head(top10_global_deaths_per_1k, 10))
```

For the top 10 countries ordered by deaths, we see that the regression is negative, where the lower the 7-day average cases the higher the 7-day average deaths.

```{r graph 2, message=FALSE}

top10_global_graph_builder(head(top10_global_cases_per_1k, 10))
```

For the top 10 countries ordered by cases, we see that the regression is positive, where the lower the 7-day average cases the lower the 7-day average deaths.

That makes sense, since in the beginning of any pandemic, the number of deaths to cases ratio was high, and as the number of cases increases gradually through time, the number of deaths decreases. We actually, lived through that, where we saw in the beginning of the pandemic a lot of people died compared to the number of cases, and as the time passed, although the number of cases increased, we saw a drastic dip in the number of deaths, especially when measures were enforced and vaccines were developed.


#### **Analysis 9:** Global - Map view of total cases and deaths.
    
```{r visualization of total deaths and cases on the world map}
global_cases_deaths_with_coord <- global_cases_deaths_with_pop %>%
  group_by(Country) %>%
  reframe(Long,
          Lat,
          Total_Deaths = max(Deaths),
          Total_Cases = max(Cases)) %>%
  distinct() %>%
  mutate(Country = ifelse(Country == "US", "United States of America", Country))  # Changing the name of the US to map correctly with the rnaturalearth geospatia data.
global_cases_deaths_with_coord
```

  - World map distribution by cases:

```{r map by cases, warning=FALSE}

# Creating a dataframe that contains unique rows for each country:
summarized_df <- global_cases_deaths_with_coord %>%
  group_by(Country) %>%
  reframe(Total_Cases = max(Total_Cases), 
            Total_Deaths = max(Total_Deaths))

# Getting the world map data:
world_map <- ne_countries(scale = 'medium', returnclass = 'sf')

# Merging the summarized dataframe with the world map dataframe:
world_map_stats_joined <- world_map %>%
  left_join(summarized_df, by = c('name' = 'Country'))

# Creating the map view:
ggplot(world_map_stats_joined) +
  geom_sf(aes(fill = Total_Cases), color = 'black', size = 0.1) +
  scale_fill_viridis_c(name = 'Total Cases', 
                       na.value = 'lightgrey', 
                       trans = 'log10',
                       labels = label_number(scale_cut = cut_short_scale())) +
  theme_minimal() +
  coord_sf(xlim = c(-180, 180), ylim = c(-90, 90)) +  # Full world map
  labs(title = 'Global Total Cases')
```

Looking at the cases we see that the US has the highest number worldwide.

Brazil, India, France, Germany and Russia have the second highest number of cases.

China, Australia and Canada come after Brazil, India, France, Germany and Russia. This is an interesting find for the number of cases, since the virus originated in China, but that indicates that measures were enforced there to reduce the infection.
  
  - World map distribution by deaths:

```{r map by deaths, warning=FALSE}

# Creating the map view:
ggplot(world_map_stats_joined) +
  geom_sf(aes(fill = Total_Deaths), color = 'black', size = 0.1) +
  scale_fill_distiller(
    name = 'Total Deaths', 
    palette = "RdYlBu",  # Red-Yellow-Blue color palette
    direction = -1,      # Reversing the palette for better visualization
    na.value = 'lightgrey', 
    trans = 'log10',
    labels = label_number(scale_cut = cut_short_scale())
  ) +
  theme_minimal() +
  coord_sf(xlim = c(-180, 180), ylim = c(-90, 90)) +  # Full world map
  labs(title = 'Global Total Deaths')
```

Looking into the death numbers, the US is not taking the first place alone, we have Brazil sharing with the US a very close total number of deaths.

Russia and India come after the the US and Brazil.

China is interestingly ranked close to the majority of the European countries; this shows that these countries took the right measures and applied the vaccine to a wider number of people, which helped reducing the death toll dramatically.

**PS:** Some of the countries you see in both maps are colored in grey; the reason for that is either some of these countries do not exist in the data or the names do not match the data in the "rnaturalearth" package.


#### **Analysis 10:** Global vs US states comparison - w/ visualization.

```{r global vs us states comparison}

# Creating a function that builds the required dataframe and sorts it according to the passed variable:
build_state_vs_country_dataframe <- function(state_df, country_df, stat_column) {
  # Creating the updated state dataframe:
  updated_states_dataframe <- state_df %>%
  ungroup() %>%
  reframe(State,
          Land_Type = "State",
          Total_Deaths,
          Total_Cases)
  
  # Creating the updated country dataframe:
  updated_countries_dataframe <- country_df %>%
    group_by(Country, Total_Deaths, Total_Cases) %>%
    reframe(Land_Type = "Country")
  
  # Creating the required dataframe containing the states and countries:
  states_countries_dataframe <- updated_countries_dataframe %>%
    full_join(updated_states_dataframe, by = join_by(Land_Type, Total_Deaths, Total_Cases)) %>%
    pivot_longer(-c("Total_Deaths", "Total_Cases", "Land_Type"), names_to = "Type", values_to = "Land") %>%
    na.omit(Land) %>%
    select(Land, Type, Total_Deaths, Total_Cases) %>%
    arrange(desc(!!sym(stat_column)))
  
  return(states_countries_dataframe)
}
```

```{r creating and showing the dataframes - analysis 10}

# Sorting by total deaths:
sorted_by_total_deaths_df <- build_state_vs_country_dataframe(states_cases_deaths_per_100K, global_cases_deaths_with_coord, "Total_Deaths")
sorted_by_total_deaths_df

# Sorting by total cases:
sorted_by_total_cases_df <- build_state_vs_country_dataframe(states_cases_deaths_per_100K, global_cases_deaths_with_coord, "Total_Cases")
sorted_by_total_cases_df
```

Let's visualize how the US states total deaths and cases compare to countries:

```{r visualization function - analysis 10}

# Creating a function that build a visualization for the top 20 land by deaths and cases:
build_top20_graph <- function(dataframe, stat_column) {
  # Creating the required bar plot:
  ggplot(dataframe %>%
         mutate(Rank = row_number()) %>%  # Adding a ranking column
         slice(1:20), 
         aes(x = reorder(Land, -!!sym(stat_column)), y = !!sym(stat_column), fill = Type)) +
      geom_bar(stat = "identity") +
      geom_text(aes(label = Rank), vjust = -0.5, size = 2.5) +  # Adding the rankings above each bar
      theme_minimal() +
      theme(
            axis.text.x = element_text(angle = 90, hjust = 1),  # Rotate x-axis labels
            legend.title = element_blank(),  # Removing legend title
            plot.title = element_text(size = 14),  # Increasing title font size
            axis.text = element_text(size = 8)  # Increasing axis label font size
           ) +
      scale_y_continuous(labels = comma) +  # Scaling the y-axis numbers to a more readable format
      labs(
          title = paste("Top 20 Lands by", stat_column),
          x = "Land",
          y = stat_column
      )
}
```

```{r creating and dsiplaying visualization - analysis 10}

# By total deaths:
build_top20_graph(sorted_by_total_deaths_df, "Total_Deaths")

# By total cases:
build_top20_graph(sorted_by_total_cases_df, "Total_Cases")
```

Looking at the top 20 list of countries for the total number of deaths, we see that a state such as California is compared to countries and ranked 20 in the list.

We can also see that around 9% of the total deaths in the US are in California.

As for the total number of cases, we have the state of California at rank 13 and Texas and Florida at rank 18 and 19 respectively.

The total cases graph also shows that California's cases constitute 11.68% of the total cases in the US, Texas's cases constitute 8.15% and Florida 7.29%.

<br>

## **Prediction Model**

In the following section, I will create a simple regression model that predicts the total number of deaths per 1k people.

First, let's see the difference between using a linear regression model and a polynomial regression model, and decide which model to use:

```{r comparision between linear regression and polynimial regression}

ggplot(top10_global_deaths_per_1k, aes(Total_Cases_per_1k, Total_Deaths_per_1k) ) + 
  geom_point() + 
  stat_smooth(method = lm, formula = y ~ poly(x, 10, raw = TRUE)) +  # Polynomial regression line
  stat_smooth(method = lm, formula = y ~ x, color = "red") +  # Linear regression line
  labs(title = "Linear vs Polynomial Regression")
```

We can see clearly that the polynomial regression line (red) fits the data points better than the linear regression line (blue); based on this result, I will use the polynomial regression model:

```{r prediction model}

# Using a Polynomial Regression Model to fit and predict the total deaths in 1k people:
mod <- lm(Total_Deaths_per_1k ~ poly(Total_Cases_per_1k, 10, raw = TRUE), data = top10_global_deaths_per_1k)
summary(mod)

# Predicting the total deaths per 1k using the fitted model:
pred_target <- predict(mod, top10_global_deaths_per_1k, type = "response")

# Appending the predicted values as a column into the dataframe:
pred_top10_global_deaths_per_1k <- top10_global_deaths_per_1k %>%
  mutate(Pred_Deaths_per_1k = round(pred_target, 4))
pred_top10_global_deaths_per_1k
```

Now, I want to compare the actual total deaths per 1k people to the predicted through a visualization:

```{r visualization - comparison between actual and predicted}

pred_top10_global_deaths_per_1k %>%
  ggplot() +
  geom_point(aes(Total_Cases_per_1k, Total_Deaths_per_1k), color = "blue") +
  geom_point(aes(Total_Cases_per_1k, Pred_Deaths_per_1k), color = "red")
```

Looking at the graph above, we see that the model performs well in some areas, especially in the beginning, excluding few outliers, but then started losing the accuracy and under fit in the middle for the points that are between 200 and 500 on the x-axis, and then improved on the points between 500 and 700.

<br>

## **Conclusion**

One can analyse many aspects in these original datasets and create different machine learning models, however, the reason I chose to analyse these metrics as you have seen in this document, is purely based on personal curiosity to understand how this pandemic impacted the whole world, on a daily and weekly basis with different population proportions, and how the number of deaths reduced through time as more measures were enforced and vaccines being developed and applied.

The model I used is a simple one and could be improved in many ways, such as:

  - Training on more data by extracting and merging additional datasets from different sources.
  
  - Further parameters tweaking.
  
  - Trying different models.
  
  - Etc.

I know it is wishful thinking, but I hope the world will never see such a disaster again, however, when it happens, we are now more prepared and equipped in many aspects to respond and mitigate faster and in a more organized manner.

<br>


